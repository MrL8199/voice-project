{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import hmmlearn.hmm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(file_path):\n",
    "    y, sr = librosa.load(file_path)  # read .wav file\n",
    "    hop_length = math.floor(sr*0.010)  # 10ms hop\n",
    "    win_length = math.floor(sr*0.025)  # 25ms frame\n",
    "    # mfcc is 12 x T matrix\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y, sr, n_mfcc=12, n_fft=1024,\n",
    "        hop_length=hop_length, win_length=win_length)\n",
    "    # substract mean from mfcc --> normalize mfcc\n",
    "    mfcc = mfcc - np.mean(mfcc, axis=1).reshape((-1, 1))\n",
    "    # delta feature 1st order and 2nd order\n",
    "    delta1 = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    # X is 36 x T\n",
    "    X = np.concatenate([mfcc, delta1, delta2], axis=0)  # O^r\n",
    "    # return T x 36 (transpose of X)\n",
    "    return X.T  # hmmlearn use T x N matrix\n",
    "\n",
    "def get_class_data(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "    mfcc = [get_mfcc(os.path.join(data_dir, f))\n",
    "            for f in files if f.endswith(\".wav\")]\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def clustering(X, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50,\n",
    "                    random_state=0, verbose=0)\n",
    "    kmeans.fit(X)\n",
    "    print(\"centers\", kmeans.cluster_centers_.shape)\n",
    "    return kmeans\n",
    "\n",
    "def initByBakis(nComp, bakisLevel):\n",
    "        ''' init start_prob and transmat_prob by Bakis model ''' \n",
    "        startprobPrior = np.zeros(nComp)\n",
    "        startprobPrior[0 : bakisLevel - 1] = 1./ (bakisLevel - 1)\n",
    "         \n",
    "        transmatPrior = getTransmatPrior(nComp, bakisLevel)\n",
    "         \n",
    "        return startprobPrior, transmatPrior\n",
    "\n",
    "def getTransmatPrior(nComp, bakisLevel):\n",
    "        ''' get transmat prior '''\n",
    "        transmatPrior = (1. / bakisLevel) * np.eye(nComp)\n",
    "         \n",
    "        for i in range(nComp - (bakisLevel - 1)):\n",
    "            for j in range(bakisLevel - 1):\n",
    "                transmatPrior[i, i + j + 1] = 1. /  bakisLevel\n",
    "                 \n",
    "        for i in range(nComp - bakisLevel + 1, nComp):\n",
    "            for j in range(nComp - i -j):\n",
    "                transmatPrior[i, i + j] = 1. / (nComp - i)\n",
    "        return transmatPrior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Load cothe dataset\nLoad duoc dataset\nLoad khong dataset\nLoad nguoi dataset\nLoad trong dataset\nLoad test_cothe dataset\nLoad test_duoc dataset\nLoad test_khong dataset\nLoad test_nguoi dataset\nLoad test_trong dataset\nvectors (13684, 36)\ncenters (10, 36)\ncenters (10, 36)\n"
    }
   ],
   "source": [
    "class_names = [\"cothe\", \"duoc\", \"khong\", \"nguoi\", \"trong\", \"test_cothe\", \"test_duoc\", \"test_khong\", \"test_nguoi\", \"test_trong\"]\n",
    "dataset = {}\n",
    "for cname in class_names:\n",
    "    print(f\"Load {cname} dataset\")\n",
    "    dataset[cname] = get_class_data(os.path.join(\"data/cutted\", cname))\n",
    "\n",
    "# Get all vectors in the datasets\n",
    "all_vectors = np.concatenate([np.concatenate(v, axis=0) for k, v in dataset.items()], axis=0)\n",
    "print(\"vectors\", all_vectors.shape)\n",
    "# Run K-Means algorithm to get clusters\n",
    "kmeans = clustering(all_vectors)\n",
    "print(\"centers\", kmeans.cluster_centers_.shape)\n",
    "\n",
    "for cname in class_names:\n",
    "    # convert all vectors to the cluster index\n",
    "    # dataset['one'] = [O^1, ... O^R]\n",
    "    # O^r = (c1, c2, ... ct, ... cT)\n",
    "    # O^r size T x 1\n",
    "    dataset[cname] = list([kmeans.predict(v).reshape(-1, 1) for v in dataset[cname]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "training class cothe\n(3535, 1) [44, 47, 51, 44, 47, 64, 41, 47, 41, 37, 37, 37, 37, 47, 37, 41, 37, 34, 41, 37, 47, 41, 37, 44, 47, 41, 51, 67, 37, 41, 41, 37, 37, 34, 47, 31, 67, 47, 37, 37, 44, 41, 37, 44, 34, 44, 44, 34, 47, 51, 34, 34, 34, 37, 41, 51, 27, 31, 37, 34, 47, 44, 37, 44, 34, 47, 37, 37, 44, 37, 41, 74, 44, 37, 41, 41, 41, 51, 44, 61, 37, 41, 41, 37] 84\n         1       -8428.7970             +nan\n         2       -7517.8999        +910.8970\n         3       -7448.2535         +69.6464\n         4       -7322.2182        +126.0353\n         5       -7112.6509        +209.5673\n         6       -6873.6579        +238.9930\n         7       -6640.2759        +233.3820\n         8       -6385.5218        +254.7541\n         9       -6208.0039        +177.5179\n        10       -6070.1627        +137.8412\n        11       -5851.3055        +218.8572\n        12       -5534.6434        +316.6621\n        13       -5346.8037        +187.8397\n        14       -5275.5423         +71.2614\n        15       -5237.5377         +38.0046\n        16       -5212.4171         +25.1206\n        17       -5192.1559         +20.2612\n        18       -5173.3193         +18.8366\n        19       -5147.1014         +26.2179\n        20       -5092.6836         +54.4178\n        21       -5020.7158         +71.9678\n        22       -4981.8863         +38.8295\n        23       -4971.3504         +10.5359\n        24       -4966.7354          +4.6150\n        25       -4962.5251          +4.2103\n        26       -4957.7487          +4.7764\n        27       -4949.6303          +8.1184\n        28       -4917.0017         +32.6286\n        29       -4806.6783        +110.3235\n        30       -4757.5076         +49.1706\n        31       -4751.4475          +6.0601\n        32       -4749.5096          +1.9379\n        33       -4748.5996          +0.9100\n        34       -4748.0205          +0.5791\n        35       -4747.5639          +0.4567\n        36       -4747.1738          +0.3901\n        37       -4746.8365          +0.3373\n        38       -4746.5475          +0.2889\n        39       -4746.3030          +0.2445\n        40       -4746.0978          +0.2052\n        41       -4745.9260          +0.1719\n        42       -4745.7817          +0.1443\n        43       -4745.6599          +0.1218\n        44       -4745.5562          +0.1037\n        45       -4745.4671          +0.0890\n        46       -4745.3900          +0.0771\n        47       -4745.3226          +0.0674\n        48       -4745.2633          +0.0593\n        49       -4745.2107          +0.0526\n        50       -4745.1637          +0.0470\n        51       -4745.1214          +0.0422\n        52       -4745.0833          +0.0382\n        53       -4745.0486          +0.0347\n        54       -4745.0170          +0.0316\n        55       -4744.9880          +0.0290\n        56       -4744.9613          +0.0267\n        57       -4744.9366          +0.0247\n        58       -4744.9136          +0.0229\n        59       -4744.8923          +0.0214\n        60       -4744.8723          +0.0200\n        61       -4744.8536          +0.0187\n        62       -4744.8359          +0.0176\n        63       -4744.8193          +0.0167\n        64       -4744.8034          +0.0158\n        65       -4744.7884          +0.0150\n        66       -4744.7741          +0.0143\n        67       -4744.7604          +0.0137\n        68       -4744.7472          +0.0132\n        69       -4744.7345          +0.0127\n        70       -4744.7223          +0.0122\n        71       -4744.7105          +0.0118\n        72       -4744.6990          +0.0115\n        73       -4744.6879          +0.0112\n        74       -4744.6770          +0.0109\n        75       -4744.6663          +0.0107\n        76       -4744.6559          +0.0104\n        77       -4744.6456          +0.0103\n        78       -4744.6355          +0.0101\n        79       -4744.6255          +0.0100\ntraining class duoc\n(2013, 1) [21, 17, 24, 37, 27, 24, 24, 24, 17, 31, 27, 17, 21, 21, 21, 24, 14, 21, 24, 27, 24, 27, 31, 27, 24, 24, 17, 14, 24, 17, 27, 24, 24, 21, 14, 17, 24, 17, 31, 21, 27, 27, 17, 17, 17, 21, 31, 17, 24, 27, 24, 27, 24, 21, 21, 21, 41, 24, 21, 34, 57, 31, 27, 27, 21, 21, 21, 24, 21, 17, 27, 17, 21, 21, 27, 21, 27, 24, 21, 21, 24, 24, 24, 21, 31] 85\n         1       -4774.0069             +nan\n         2       -4259.8865        +514.1204\n         3       -4211.3621         +48.5244\n         4       -4144.5231         +66.8390\n         5       -4069.4024         +75.1208\n         6       -4007.5418         +61.8605\n         7       -3963.1019         +44.4400\n         8       -3913.8229         +49.2790\n         9       -3830.0604         +83.7624\n        10       -3707.6758        +122.3847\n        11       -3597.8231        +109.8527\n        12       -3519.0812         +78.7419\n        13       -3423.7000         +95.3812\n        14       -3295.8784        +127.8216\n        15       -3184.6657        +111.2128\n        16       -3115.0807         +69.5849\n        17       -3076.3887         +38.6920\n        18       -3055.4357         +20.9530\n        19       -3040.5696         +14.8661\n        20       -3027.2522         +13.3173\n        21       -3016.4141         +10.8381\n        22       -3009.5751          +6.8390\n        23       -3006.5780          +2.9972\n        24       -3005.4656          +1.1123\n        25       -3004.7348          +0.7308\n        26       -3004.3940          +0.3408\n        27       -3004.2200          +0.1741\n        28       -3004.1205          +0.0995\n        29       -3004.0558          +0.0647\n        30       -3004.0081          +0.0477\n        31       -3003.9694          +0.0387\n        32       -3003.9358          +0.0335\n        33       -3003.9056          +0.0302\n        34       -3003.8778          +0.0278\n        35       -3003.8517          +0.0261\n        36       -3003.8267          +0.0250\n        37       -3003.8021          +0.0246\n        38       -3003.7771          +0.0250\n        39       -3003.7506          +0.0265\n        40       -3003.7213          +0.0293\n        41       -3003.6872          +0.0341\n        42       -3003.6456          +0.0416\n        43       -3003.5926          +0.0529\n        44       -3003.5229          +0.0697\n        45       -3003.4296          +0.0934\n        46       -3003.3056          +0.1240\n        47       -3003.1487          +0.1568\n        48       -3002.9690          +0.1797\n        49       -3002.7892          +0.1798\n        50       -3002.6319          +0.1573\n        51       -3002.5059          +0.1260\n        52       -3002.4080          +0.0979\n        53       -3002.3314          +0.0766\n        54       -3002.2701          +0.0612\n        55       -3002.2201          +0.0500\n        56       -3002.1787          +0.0415\n        57       -3002.1439          +0.0348\n        58       -3002.1145          +0.0294\n        59       -3002.0896          +0.0249\n        60       -3002.0684          +0.0212\n        61       -3002.0503          +0.0180\n        62       -3002.0350          +0.0153\n        63       -3002.0220          +0.0130\n        64       -3002.0112          +0.0109\n        65       -3002.0022          +0.0090\n         1       -5222.1696             +nantraining class khong\n(2117, 1) [44, 21, 21, 24, 31, 21, 24, 24, 21, 21, 24, 27, 21, 21, 31, 24, 21, 27, 14, 24, 24, 27, 24, 34, 24, 31, 27, 21, 27, 24, 27, 31, 47, 24, 21, 24, 24, 34, 27, 17, 21, 27, 24, 24, 17, 31, 21, 24, 17, 21, 21, 27, 24, 31, 41, 31, 24, 27, 24, 24, 24, 21, 24, 21, 24, 27, 21, 24, 24, 27, 24, 17, 21, 17, 24, 31, 41, 24, 17, 21, 21, 31, 27, 17, 21] 85\n\n         2       -4305.3456        +916.8241\n         3       -4179.9757        +125.3698\n         4       -3982.6148        +197.3610\n         5       -3807.9815        +174.6333\n         6       -3687.0645        +120.9170\n         7       -3532.2236        +154.8409\n         8       -3350.1138        +182.1098\n         9       -3217.7187        +132.3952\n        10       -3113.2205        +104.4981\n        11       -3034.1377         +79.0828\n        12       -2999.9928         +34.1449\n        13       -2987.9610         +12.0319\n        14       -2980.3638          +7.5972\n        15       -2973.8382          +6.5255\n        16       -2967.9060          +5.9322\n        17       -2962.9762          +4.9298\n        18       -2959.1627          +3.8134\n        19       -2956.1303          +3.0324\n        20       -2953.4972          +2.6331\n        21       -2951.1180          +2.3792\n        22       -2948.7519          +2.3661\n        23       -2946.0075          +2.7444\n        24       -2942.6555          +3.3520\n        25       -2939.1654          +3.4902\n        26       -2935.9662          +3.1992\n        27       -2932.8869          +3.0793\n        28       -2930.6151          +2.2718\n        29       -2929.7131          +0.9021\n        30       -2929.6893          +0.0237\n        31       -2930.5400          -0.8507\n         1       -4138.2215             +nan\ntraining class nguoi\n(1739, 1) [22, 20, 26, 32, 27, 20, 29, 20, 37, 22, 34, 25, 17, 17, 16, 18, 24, 26, 28, 20, 22, 22, 13, 21, 21, 23, 23, 21, 24, 15, 17, 18, 28, 17, 23, 19, 19, 25, 23, 32, 27, 22, 17, 18, 32, 19, 22, 28, 28, 39, 19, 22, 19, 16, 25, 17, 33, 22, 20, 28, 13, 24, 24, 28, 50, 36, 26, 18, 20, 19, 19, 20, 22, 23, 18] 75\n         2       -3112.5902       +1025.6314\n         3       -3071.5950         +40.9951\n         4       -3020.4655         +51.1295\n         5       -2959.5388         +60.9268\n         6       -2880.8001         +78.7387\n         7       -2776.2981        +104.5020\n         8       -2660.0416        +116.2565\n         9       -2494.7685        +165.2731\n        10       -2370.2968        +124.4718\n        11       -2327.1758         +43.1210\n        12       -2293.2227         +33.9530\n        13       -2236.8664         +56.3564\n        14       -2113.9611        +122.9053\n        15       -1963.8035        +150.1575\n        16       -1912.8732         +50.9304\n        17       -1896.9616         +15.9116\n        18       -1892.8144          +4.1472\n        19       -1891.9580          +0.8564\n        20       -1891.6846          +0.2734\n        21       -1891.5173          +0.1673\n        22       -1891.3742          +0.1431\n        23       -1891.3029          +0.0713\n        24       -1891.3480          -0.0451\n         1       -5538.7809             +nantraining class trong\n(2267, 1) [27, 34, 31, 24, 27, 27, 31, 47, 24, 24, 21, 24, 31, 34, 27, 21, 37, 24, 31, 27, 31, 21, 27, 21, 24, 24, 44, 37, 34, 34, 24, 31, 24, 31, 21, 24, 24, 21, 17, 21, 24, 21, 24, 27, 27, 51, 21, 14, 31, 21, 27, 21, 24, 24, 34, 27, 24, 31, 27, 24, 31, 27, 27, 24, 24, 24, 27, 21, 31, 27, 17, 24, 31, 24, 24, 44, 24, 24, 27, 21, 57, 24, 31] 83\n\n         2       -4726.8969        +811.8840\n         3       -4612.2869        +114.6100\n         4       -4448.0067        +164.2802\n         5       -4296.2596        +151.7470\n         6       -4184.4984        +111.7613\n         7       -4067.0101        +117.4883\n         8       -3917.6268        +149.3834\n         9       -3732.2024        +185.4244\n        10       -3619.8122        +112.3902\n        11       -3494.8894        +124.9229\n        12       -3276.1797        +218.7096\n        13       -3073.0091        +203.1707\n        14       -3001.2242         +71.7849\n        15       -2978.3803         +22.8438\n        16       -2964.1724         +14.2080\n        17       -2952.6042         +11.5682\n        18       -2947.0429          +5.5613\n        19       -2944.1265          +2.9164\n        20       -2941.3936          +2.7329\n        21       -2939.4611          +1.9325\nTraining done\nTesting\n        22       -2938.2350          +1.2261\n        23       -2937.7021          +0.5329\n        24       -2938.4340          -0.7319\n"
    }
   ],
   "source": [
    "models = {}\n",
    "for cname in class_names:\n",
    "    class_vectors = dataset[cname]\n",
    "    nComp = 5\n",
    "    startprobPrior,transmatPrior = initByBakis(nComp=nComp,bakisLevel=3)\n",
    "    # startprobPrior = np.array([0.7, 0.2, 0.1, 0.0, 0.0])    \n",
    "    # transmatPrior = np.array([\n",
    "    #     [0.1, 0.7, 0.0, 0.3, 0.0],\n",
    "    #     [0.7, 0.1, 0.0, 0.0, 0.0],   \n",
    "    #     [0.0, 0.0, 0.1, 0.7, 0.3],\n",
    "    #     [0.0, 0.0, 0.0, 0.1, 0.7],\n",
    "    #     [0.3, 0.0, 0.0, 0.0, 0.1],\n",
    "    # ])\n",
    "\n",
    "    hmm = hmmlearn.hmm.MultinomialHMM(\n",
    "        n_components=nComp, random_state=0, n_iter=1000, verbose=True,\n",
    "        startprob_prior=startprobPrior,\n",
    "        transmat_prior=transmatPrior,\n",
    "        init_params='e',\n",
    "        params='ste'\n",
    "    )\n",
    "    if cname[:4] != 'test':\n",
    "        X = np.concatenate(dataset[cname])\n",
    "        lengths = list([len(x) for x in dataset[cname]])\n",
    "        print(\"training class\", cname)\n",
    "        print(X.shape, lengths, len(lengths))\n",
    "        hmm.fit(X, lengths=lengths)\n",
    "        models[cname] = hmm\n",
    "print(\"Training done\")\n",
    "print(\"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\nAccuracy: {'test_cothe': 1.0, 'test_duoc': 0.8571428571428571, 'test_khong': 0.8, 'test_nguoi': 0.9333333333333333, 'test_trong': 1.0}\n"
    }
   ],
   "source": [
    "accuracy = {}\n",
    "for cname in class_names:\n",
    "    if cname[:4] != 'test':\n",
    "        continue\n",
    "    total_data = len(dataset[cname])\n",
    "    true_cnt = 0\n",
    "    # true result là tên chính xác của bộ test\n",
    "    true_result = class_names[class_names.index(cname) % 5]\n",
    "    for O in dataset[cname]:\n",
    "        score = {cname: model.score(O, [len(O)]) for cname, model in models.items() if cname[:4] != 'test'}\n",
    "        result = max(score, key=lambda k: score[k])\n",
    "        isTrue = true_result == result\n",
    "        # print(cname, score, result, isTrue)\n",
    "        if isTrue:\n",
    "            true_cnt += 1\n",
    "    accuracy[cname] = true_cnt/total_data\n",
    "print(\"\\n\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Saved!\n"
    }
   ],
   "source": [
    "with open(\"multinomial_hmm.pkl\", \"wb\") as file:\n",
    "    pickle.dump(models, file)\n",
    "pickle.dump(kmeans, open(\"kmeans.pkl\", \"wb\"))\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}