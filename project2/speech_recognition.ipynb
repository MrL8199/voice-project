{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import hmmlearn.hmm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(file_path):\n",
    "    y, sr = librosa.load(file_path)  # read .wav file\n",
    "    hop_length = math.floor(sr*0.010)  # 10ms hop\n",
    "    win_length = math.floor(sr*0.025)  # 25ms frame\n",
    "    # mfcc is 12 x T matrix\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y, sr, n_mfcc=12, n_fft=1024,\n",
    "        hop_length=hop_length, win_length=win_length)\n",
    "    # substract mean from mfcc --> normalize mfcc\n",
    "    mfcc = mfcc - np.mean(mfcc, axis=1).reshape((-1, 1))\n",
    "    # delta feature 1st order and 2nd order\n",
    "    delta1 = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    # X is 36 x T\n",
    "    X = np.concatenate([mfcc, delta1, delta2], axis=0)  # O^r\n",
    "    # return T x 36 (transpose of X)\n",
    "    return X.T  # hmmlearn use T x N matrix\n",
    "\n",
    "def get_class_data(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "    mfcc = [get_mfcc(os.path.join(data_dir, f))\n",
    "            for f in files if f.endswith(\".wav\")]\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def clustering(X, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50,\n",
    "                    random_state=0, verbose=0)\n",
    "    kmeans.fit(X)\n",
    "    print(\"centers\", kmeans.cluster_centers_.shape)\n",
    "    return kmeans\n",
    "\n",
    "def initByBakis(nComp, bakisLevel):\n",
    "        ''' init start_prob and transmat_prob by Bakis model ''' \n",
    "        startprobPrior = np.zeros(nComp)\n",
    "        startprobPrior[0 : bakisLevel - 1] = 1./ (bakisLevel - 1)\n",
    "         \n",
    "        transmatPrior = getTransmatPrior(nComp, bakisLevel)\n",
    "         \n",
    "        return startprobPrior, transmatPrior\n",
    "\n",
    "def getTransmatPrior(nComp, bakisLevel):\n",
    "        ''' get transmat prior '''\n",
    "        transmatPrior = (1. / bakisLevel) * np.eye(nComp)\n",
    "         \n",
    "        for i in range(nComp - (bakisLevel - 1)):\n",
    "            for j in range(bakisLevel - 1):\n",
    "                transmatPrior[i, i + j + 1] = 1. /  bakisLevel\n",
    "                 \n",
    "        for i in range(nComp - bakisLevel + 1, nComp):\n",
    "            for j in range(nComp - i -j):\n",
    "                transmatPrior[i, i + j] = 1. / (nComp - i)\n",
    "        return transmatPrior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Load cothe dataset\nLoad duoc dataset\nLoad khong dataset\nLoad nguoi dataset\nLoad trong dataset\nLoad test_cothe dataset\nLoad test_duoc dataset\nLoad test_khong dataset\nLoad test_nguoi dataset\nLoad test_trong dataset\nvectors (13684, 36)\ncenters (10, 36)\ncenters (10, 36)\n"
    }
   ],
   "source": [
    "class_names = [\"cothe\", \"duoc\", \"khong\", \"nguoi\", \"trong\", \"test_cothe\", \"test_duoc\", \"test_khong\", \"test_nguoi\", \"test_trong\"]\n",
    "dataset = {}\n",
    "for cname in class_names:\n",
    "    print(f\"Load {cname} dataset\")\n",
    "    dataset[cname] = get_class_data(os.path.join(\"data/cutted\", cname))\n",
    "\n",
    "# Get all vectors in the datasets\n",
    "all_vectors = np.concatenate([np.concatenate(v, axis=0) for k, v in dataset.items()], axis=0)\n",
    "print(\"vectors\", all_vectors.shape)\n",
    "# Run K-Means algorithm to get clusters\n",
    "kmeans = clustering(all_vectors)\n",
    "print(\"centers\", kmeans.cluster_centers_.shape)\n",
    "\n",
    "for cname in class_names:\n",
    "    # convert all vectors to the cluster index\n",
    "    # dataset['one'] = [O^1, ... O^R]\n",
    "    # O^r = (c1, c2, ... ct, ... cT)\n",
    "    # O^r size T x 1\n",
    "    dataset[cname] = list([kmeans.predict(v).reshape(-1, 1) for v in dataset[cname]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "training class cothe\n(3535, 1) [44, 47, 51, 44, 47, 64, 41, 47, 41, 37, 37, 37, 37, 47, 37, 41, 37, 34, 41, 37, 47, 41, 37, 44, 47, 41, 51, 67, 37, 41, 41, 37, 37, 34, 47, 31, 67, 47, 37, 37, 44, 41, 37, 44, 34, 44, 44, 34, 47, 51, 34, 34, 34, 37, 41, 51, 27, 31, 37, 34, 47, 44, 37, 44, 34, 47, 37, 37, 44, 37, 41, 74, 44, 37, 41, 41, 41, 51, 44, 61, 37, 41, 41, 37] 84\n         1       -8088.3895             +nan\n         2       -7505.7971        +582.5924\n         3       -7413.5217         +92.2754\n         4       -7213.0626        +200.4592\n         5       -6754.7256        +458.3370\n         6       -6089.4117        +665.3138\n         7       -5669.2160        +420.1958\n         8       -5294.9880        +374.2280\n         9       -4830.9172        +464.0708\n        10       -4552.2772        +278.6400\n        11       -4402.6200        +149.6572\n        12       -4307.4358         +95.1842\n        13       -4225.7193         +81.7165\n        14       -4172.7573         +52.9621\n        15       -4135.9644         +36.7928\n        16       -4097.6867         +38.2777\n        17       -4052.1840         +45.5027\n        18       -4014.2800         +37.9040\n        19       -3987.8974         +26.3826\n        20       -3970.3570         +17.5404\n        21       -3962.2151          +8.1419\n        22       -3956.3015          +5.9136\n        23       -3952.4927          +3.8088\n        24       -3949.9338          +2.5589\n        25       -3949.2847          +0.6491\n        26       -3948.9348          +0.3499\n        27       -3949.0224          -0.0876\n         1       -4674.1179             +nan\n         2       -4282.0634        +392.0545\ntraining class duoc\n(2013, 1) [21, 17, 24, 37, 27, 24, 24, 24, 17, 31, 27, 17, 21, 21, 21, 24, 14, 21, 24, 27, 24, 27, 31, 27, 24, 24, 17, 14, 24, 17, 27, 24, 24, 21, 14, 17, 24, 17, 31, 21, 27, 27, 17, 17, 17, 21, 31, 17, 24, 27, 24, 27, 24, 21, 21, 21, 41, 24, 21, 34, 57, 31, 27, 27, 21, 21, 21, 24, 21, 17, 27, 17, 21, 21, 27, 21, 27, 24, 21, 21, 24, 24, 24, 21, 31] 85\n         3       -4255.7799         +26.2835\n         4       -4191.4007         +64.3791\n         5       -4045.2558        +146.1449\n         6       -3881.4323        +163.8236\n         7       -3718.1777        +163.2546\n         8       -3462.6298        +255.5478\n         9       -3165.2190        +297.4109\n        10       -2963.4551        +201.7639\n        11       -2882.5755         +80.8796\n        12       -2843.1587         +39.4168\n        13       -2805.2907         +37.8679\n        14       -2757.5543         +47.7364\n        15       -2735.4646         +22.0897\n        16       -2729.1379          +6.3267\n        17       -2723.8219          +5.3160\n        18       -2718.2564          +5.5654\n        19       -2709.5052          +8.7513\n        20       -2693.8691         +15.6360\n        21       -2676.4629         +17.4063\n        22       -2663.9912         +12.4716\n        23       -2657.8502          +6.1410\n        24       -2654.4192          +3.4310\n        25       -2652.1470          +2.2722\n        26       -2650.1976          +1.9494\n        27       -2648.3467          +1.8509\n        28       -2647.7471          +0.5997\n        29       -2646.2407          +1.5064\n        30       -2644.6416          +1.5990\n        31       -2643.5130          +1.1287\n        32       -2641.7863          +1.7267\n        33       -2640.7586          +1.0277\n        34       -2640.1246          +0.6340\n        35       -2639.4653          +0.6593\n        36       -2638.8208          +0.6445\n        37       -2638.2741          +0.5467\n        38       -2637.8396          +0.4345\n        39       -2637.4791          +0.3605\n        40       -2637.1285          +0.3506\n        41       -2636.7154          +0.4131\n        42       -2636.2051          +0.5103\n        43       -2635.6725          +0.5326\n        44       -2635.2430          +0.4295\n        45       -2634.9325          +0.3105\n        46       -2634.6653          +0.2672\n        47       -2634.3851          +0.2802\n        48       -2634.1196          +0.2655\n        49       -2633.9125          +0.2070\n        50       -2633.7501          +0.1625\n        51       -2633.6082          +0.1419\n        52       -2633.4750          +0.1332\n        53       -2633.3463          +0.1286\n        54       -2633.2225          +0.1239\n        55       -2633.1060          +0.1165\n        56       -2633.0000          +0.1060\n        57       -2632.9066          +0.0934\n        58       -2632.8265          +0.0801\n        59       -2632.7592          +0.0673\n        60       -2632.7035          +0.0558\n        61       -2632.6575          +0.0460\n        62       -2632.6197          +0.0378\n        63       -2632.5884          +0.0313\n        64       -2632.5623          +0.0261\n        65       -2632.5403          +0.0220\n        66       -2632.5215          +0.0188\n        67       -2632.5051          +0.0164\n        68       -2632.4905          +0.0146\n        69       -2632.4772          +0.0133\n        70       -2632.4647          +0.0124\n        71       -2632.4529          +0.0119\n        72       -2632.4413          +0.0116\n        73       -2632.4298          +0.0115\n        74       -2632.4182          +0.0116\n        75       -2632.4064          +0.0118\n        76       -2632.3944          +0.0120\n        77       -2632.3821          +0.0123\n        78       -2632.3696          +0.0125\n        79       -2632.3570          +0.0126\n        80       -2632.3444          +0.0126\n        81       -2632.3320          +0.0124\n        82       -2632.3200          +0.0120\n        83       -2632.3084          +0.0115\n        84       -2632.2975          +0.0109\n        85       -2632.2874          +0.0101\n        86       -2632.2781          +0.0093\n         1       -4830.0343             +nan\n         2       -4363.0309        +467.0034\n         3       -4329.9150         +33.1158\ntraining class khong\n(2117, 1) [44, 21, 21, 24, 31, 21, 24, 24, 21, 21, 24, 27, 21, 21, 31, 24, 21, 27, 14, 24, 24, 27, 24, 34, 24, 31, 27, 21, 27, 24, 27, 31, 47, 24, 21, 24, 24, 34, 27, 17, 21, 27, 24, 24, 17, 31, 21, 24, 17, 21, 21, 27, 24, 31, 41, 31, 24, 27, 24, 24, 24, 21, 24, 21, 24, 27, 21, 24, 24, 27, 24, 17, 21, 17, 24, 31, 41, 24, 17, 21, 21, 31, 27, 17, 21] 85\n         4       -4258.8704         +71.0446\n         5       -4105.0289        +153.8415\n         6       -3908.2122        +196.8167\n         7       -3736.7257        +171.4865\n         8       -3511.7042        +225.0215\n         9       -3247.5785        +264.1257\n        10       -3087.0877        +160.4908\n        11       -3006.3097         +80.7779\n        12       -2930.6895         +75.6203\n        13       -2836.1646         +94.5249\n        14       -2742.4531         +93.7115\n        15       -2687.8646         +54.5885\n        16       -2660.0962         +27.7684\n        17       -2642.5640         +17.5322\n        18       -2627.1453         +15.4187\n        19       -2616.9578         +10.1875\n        20       -2611.3583          +5.5994\n        21       -2610.8224          +0.5360\n        22       -2605.0816          +5.7408\n        23       -2602.8790          +2.2026\n        24       -2601.7196          +1.1594\n        25       -2600.6316          +1.0880\n        26       -2600.0890          +0.5427\n        27       -2601.1801          -1.0912\n         1       -3742.2854             +nan\n         2       -3121.7045        +620.5808\ntraining class nguoi\n(1739, 1) [22, 20, 26, 32, 27, 20, 29, 20, 37, 22, 34, 25, 17, 17, 16, 18, 24, 26, 28, 20, 22, 22, 13, 21, 21, 23, 23, 21, 24, 15, 17, 18, 28, 17, 23, 19, 19, 25, 23, 32, 27, 22, 17, 18, 32, 19, 22, 28, 28, 39, 19, 22, 19, 16, 25, 17, 33, 22, 20, 28, 13, 24, 24, 28, 50, 36, 26, 18, 20, 19, 19, 20, 22, 23, 18] 75\n         3       -3071.9730         +49.7316\n         4       -2946.9528        +125.0201\n         5       -2736.6845        +210.2683\n         6       -2545.8431        +190.8414\n         7       -2401.1379        +144.7052\n         8       -2299.9753        +101.1626\n         9       -2159.5628        +140.4125\n        10       -1981.5288        +178.0340\n        11       -1870.3952        +111.1336\n        12       -1822.2747         +48.1205\n        13       -1793.5600         +28.7147\n        14       -1763.7627         +29.7973\n        15       -1739.8698         +23.8929\n        16       -1715.6859         +24.1839\n        17       -1701.8900         +13.7959\n        18       -1697.1350          +4.7550\n        19       -1694.7996          +2.3354\n        20       -1693.1121          +1.6874\n        21       -1692.2203          +0.8918\n        22       -1691.8157          +0.4046\n        23       -1691.8517          -0.0359\n         1       -5249.9878             +nan\n         2       -4779.4080        +470.5798\n         3       -4737.2385         +42.1695training class trong\n(2267, 1) [27, 34, 31, 24, 27, 27, 31, 47, 24, 24, 21, 24, 31, 34, 27, 21, 37, 24, 31, 27, 31, 21, 27, 21, 24, 24, 44, 37, 34, 34, 24, 31, 24, 31, 21, 24, 24, 21, 17, 21, 24, 21, 24, 27, 27, 51, 21, 14, 31, 21, 27, 21, 24, 24, 34, 27, 24, 31, 27, 24, 31, 27, 27, 24, 24, 24, 27, 21, 31, 27, 17, 24, 31, 24, 24, 44, 24, 24, 27, 21, 57, 24, 31] 83\n\n         4       -4648.3012         +88.9373\n         5       -4455.4269        +192.8742\n         6       -4148.7340        +306.6929\n         7       -3775.6354        +373.0986\n         8       -3458.3153        +317.3200\n         9       -3244.8658        +213.4495\n        10       -3135.7028        +109.1630\n        11       -3073.7033         +61.9995\n        12       -3022.3165         +51.3869\n        13       -2962.0917         +60.2248\n        14       -2886.2584         +75.8333\n        15       -2775.9660        +110.2924\n        16       -2659.0543        +116.9117\n        17       -2593.3741         +65.6803\n        18       -2566.2816         +27.0925\n        19       -2557.4817          +8.7998\n        20       -2554.7544          +2.7274\n        21       -2551.0627          +3.6917\n        22       -2547.7592          +3.3035\n        23       -2541.2485          +6.5106\n        24       -2539.3627          +1.8858\n        25       -2537.9997          +1.3630\n        26       -2536.8322          +1.1675\n        27       -2536.2661          +0.5661\n        28       -2536.0629          +0.2032\n        29       -2535.8944          +0.1685\n        30       -2535.0473          +0.8471\n        31       -2533.6747          +1.3726\n        32       -2533.0839          +0.5907\n        33       -2532.7220          +0.3619\n        34       -2532.6601          +0.0619\nTraining done\nTesting\n        35       -2532.7321          -0.0720\n"
    }
   ],
   "source": [
    "models = {}\n",
    "for cname in class_names:\n",
    "    if cname[:4] != 'test':\n",
    "        class_vectors = dataset[cname]\n",
    "        if cname == 'cothe':\n",
    "            nComp = 4 * 3\n",
    "        else:\n",
    "            nComp = 3 * 3\n",
    "        startprobPrior,transmatPrior = initByBakis(nComp=nComp,bakisLevel=3)\n",
    "        hmm = hmmlearn.hmm.MultinomialHMM(\n",
    "            n_components=nComp, random_state=15, n_iter=1000, verbose=True,\n",
    "            startprob_prior=startprobPrior,\n",
    "            transmat_prior=transmatPrior,\n",
    "            init_params='ste',\n",
    "            params='ste'\n",
    "        )\n",
    "\n",
    "        X = np.concatenate(dataset[cname])\n",
    "        lengths = list([len(x) for x in dataset[cname]])\n",
    "        print(\"training class\", cname)\n",
    "        print(X.shape, lengths, len(lengths))\n",
    "        hmm.fit(X, lengths=lengths)\n",
    "        models[cname] = hmm\n",
    "print(\"Training done\")\n",
    "print(\"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\nMultinomialHMM accuracy: {'test_cothe': 1.0, 'test_duoc': 0.9285714285714286, 'test_khong': 0.8, 'test_nguoi': 1.0, 'test_trong': 0.9333333333333333}\n"
    }
   ],
   "source": [
    "accuracy = {}\n",
    "for cname in class_names:\n",
    "    if cname[:4] != 'test':\n",
    "        continue\n",
    "    total_data = len(dataset[cname])\n",
    "    true_cnt = 0\n",
    "    # true result là tên chính xác của bộ test\n",
    "    true_result = class_names[class_names.index(cname) % 5]\n",
    "    for O in dataset[cname]:\n",
    "        score = {cname: model.score(O, [len(O)]) for cname, model in models.items() if cname[:4] != 'test'}\n",
    "        result = max(score, key=lambda k: score[k])\n",
    "        isTrue = true_result == result\n",
    "        # print(cname, score, result, isTrue)\n",
    "        if isTrue:\n",
    "            true_cnt += 1\n",
    "    accuracy[cname] = true_cnt/total_data\n",
    "print(\"\\n\\nMultinomialHMM accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Saved!\n"
    }
   ],
   "source": [
    "with open(\"multinomial_hmm.pkl\", \"wb\") as file:\n",
    "    pickle.dump(models, file)\n",
    "pickle.dump(kmeans, open(\"kmeans.pkl\", \"wb\"))\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}