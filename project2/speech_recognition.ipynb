{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import hmmlearn.hmm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(file_path):\n",
    "    y, sr = librosa.load(file_path)  # read .wav file\n",
    "    hop_length = math.floor(sr*0.010)  # 10ms hop\n",
    "    win_length = math.floor(sr*0.025)  # 25ms frame\n",
    "    # mfcc is 12 x T matrix\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y, sr, n_mfcc=12, n_fft=1024,\n",
    "        hop_length=hop_length, win_length=win_length)\n",
    "    # substract mean from mfcc --> normalize mfcc\n",
    "    mfcc = mfcc - np.mean(mfcc, axis=1).reshape((-1, 1))\n",
    "    # delta feature 1st order and 2nd order\n",
    "    delta1 = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    # X is 36 x T\n",
    "    X = np.concatenate([mfcc, delta1, delta2], axis=0)  # O^r\n",
    "    # return T x 36 (transpose of X)\n",
    "    return X.T  # hmmlearn use T x N matrix\n",
    "\n",
    "def get_class_data(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "    mfcc = [get_mfcc(os.path.join(data_dir, f))\n",
    "            for f in files if f.endswith(\".wav\")]\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def clustering(X, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50,\n",
    "                    random_state=0, verbose=0)\n",
    "    kmeans.fit(X)\n",
    "    print(\"centers\", kmeans.cluster_centers_.shape)\n",
    "    return kmeans\n",
    "\n",
    "def initByBakis(nComp, bakisLevel):\n",
    "        ''' init start_prob and transmat_prob by Bakis model ''' \n",
    "        startprobPrior = np.zeros(nComp)\n",
    "        startprobPrior[0 : bakisLevel - 1] = 1./ (bakisLevel - 1)\n",
    "         \n",
    "        transmatPrior = getTransmatPrior(nComp, bakisLevel)\n",
    "         \n",
    "        return startprobPrior, transmatPrior\n",
    "\n",
    "def getTransmatPrior(nComp, bakisLevel):\n",
    "        ''' get transmat prior '''\n",
    "        transmatPrior = (1. / bakisLevel) * np.eye(nComp)\n",
    "         \n",
    "        for i in range(nComp - (bakisLevel - 1)):\n",
    "            for j in range(bakisLevel - 1):\n",
    "                transmatPrior[i, i + j + 1] = 1. /  bakisLevel\n",
    "                 \n",
    "        for i in range(nComp - bakisLevel + 1, nComp):\n",
    "            for j in range(nComp - i -j):\n",
    "                transmatPrior[i, i + j] = 1. / (nComp - i)\n",
    "        return transmatPrior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Load cothe dataset\nLoad duoc dataset\nLoad khong dataset\nLoad nguoi dataset\nLoad trong dataset\nLoad test_cothe dataset\nLoad test_duoc dataset\nLoad test_khong dataset\nLoad test_nguoi dataset\nLoad test_trong dataset\nvectors (13684, 36)\ncenters (10, 36)\ncenters (10, 36)\n"
    }
   ],
   "source": [
    "class_names = [\"cothe\", \"duoc\", \"khong\", \"nguoi\", \"trong\", \"test_cothe\", \"test_duoc\", \"test_khong\", \"test_nguoi\", \"test_trong\"]\n",
    "dataset = {}\n",
    "for cname in class_names:\n",
    "    print(f\"Load {cname} dataset\")\n",
    "    dataset[cname] = get_class_data(os.path.join(\"data/cutted\", cname))\n",
    "\n",
    "# Get all vectors in the datasets\n",
    "all_vectors = np.concatenate([np.concatenate(v, axis=0) for k, v in dataset.items()], axis=0)\n",
    "print(\"vectors\", all_vectors.shape)\n",
    "# Run K-Means algorithm to get clusters\n",
    "kmeans = clustering(all_vectors)\n",
    "print(\"centers\", kmeans.cluster_centers_.shape)\n",
    "\n",
    "for cname in class_names:\n",
    "    # convert all vectors to the cluster index\n",
    "    # dataset['one'] = [O^1, ... O^R]\n",
    "    # O^r = (c1, c2, ... ct, ... cT)\n",
    "    # O^r size T x 1\n",
    "    dataset[cname] = list([kmeans.predict(v).reshape(-1, 1) for v in dataset[cname]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.5 0.5 0.  0.  0.  0.  0. ]\n[[0.33333333 0.33333333 0.33333333 0.         0.         0.\n  0.        ]\n [0.         0.33333333 0.33333333 0.33333333 0.         0.\n  0.        ]\n [0.         0.         0.33333333 0.33333333 0.33333333 0.\n  0.        ]\n [0.         0.         0.         0.33333333 0.33333333 0.33333333\n  0.        ]\n [0.         0.         0.         0.         0.33333333 0.33333333\n  0.33333333]\n [0.         0.         0.         0.         0.         0.5\n  0.        ]\n [0.         0.         0.         0.         0.         0.\n  1.        ]]\ntraining class cothe\n(3535, 1) [44, 47, 51, 44, 47, 64, 41, 47, 41, 37, 37, 37, 37, 47, 37, 41, 37, 34, 41, 37, 47, 41, 37, 44, 47, 41, 51, 67, 37, 41, 41, 37, 37, 34, 47, 31, 67, 47, 37, 37, 44, 41, 37, 44, 34, 44, 44, 34, 47, 51, 34, 34, 34, 37, 41, 51, 27, 31, 37, 34, 47, 44, 37, 44, 34, 47, 37, 37, 44, 37, 41, 74, 44, 37, 41, 41, 41, 51, 44, 61, 37, 41, 41, 37] 84\n         1       -8058.1877             +nan\n         2       -7537.6784        +520.5093\n         3       -7494.3069         +43.3715\n         4       -7404.3983         +89.9085\n         5       -7196.7193        +207.6790\n         6       -6842.3147        +354.4046\n         7       -6376.5134        +465.8013\n         8       -5835.4247        +541.0887\n         9       -5300.8632        +534.5615\n        10       -4895.1298        +405.7334\n        11       -4754.3571        +140.7728\n        12       -4692.0625         +62.2945\n        13       -4637.1159         +54.9466\n        14       -4583.5478         +53.5682\n        15       -4510.1913         +73.3565\n        16       -4439.5570         +70.6343\n        17       -4408.1340         +31.4230\n        18       -4394.3542         +13.7798\n        19       -4385.2377          +9.1165\n        20       -4376.8376          +8.4001\n        21       -4370.9171          +5.9205\n        22       -4367.4155          +3.5016\n        23       -4365.4722          +1.9434\n        24       -4364.4415          +1.0307\n        25       -4364.0060          +0.4355\n        26       -4364.0212          -0.0152\n         1       -4671.6796             +nan\n         2       -4279.9086        +391.7710\n         3       -4247.4617         +32.4468\ntraining class duoc\n(2013, 1) [21, 17, 24, 37, 27, 24, 24, 24, 17, 31, 27, 17, 21, 21, 21, 24, 14, 21, 24, 27, 24, 27, 31, 27, 24, 24, 17, 14, 24, 17, 27, 24, 24, 21, 14, 17, 24, 17, 31, 21, 27, 27, 17, 17, 17, 21, 31, 17, 24, 27, 24, 27, 24, 21, 21, 21, 41, 24, 21, 34, 57, 31, 27, 27, 21, 21, 21, 24, 21, 17, 27, 17, 21, 21, 27, 21, 27, 24, 21, 21, 24, 24, 24, 21, 31] 85\n         4       -4162.9998         +84.4619\n         5       -3992.9316        +170.0682\n         6       -3826.4287        +166.5029\n         7       -3615.3435        +211.0852\n         8       -3299.1167        +316.2267\n         9       -3027.3507        +271.7660\n        10       -2879.3104        +148.0403\n        11       -2810.2785         +69.0319\n        12       -2784.3308         +25.9476\n        13       -2770.7051         +13.6257\n        14       -2758.6038         +12.1013\n        15       -2751.5109          +7.0929\n        16       -2747.4831          +4.0277\n        17       -2741.5862          +5.8969\n        18       -2734.1497          +7.4366\n        19       -2729.9060          +4.2437\n        20       -2728.3605          +1.5454\n        21       -2727.5305          +0.8300\n        22       -2727.2682          +0.2623\n        23       -2726.0759          +1.1922\n        24       -2722.7085          +3.3674\n        25       -2713.0317          +9.6769\n        26       -2699.1349         +13.8967\n        27       -2684.4019         +14.7331\n        28       -2676.7804          +7.6214\n        29       -2674.6850          +2.0954\n        30       -2673.9501          +0.7349\n        31       -2673.6963          +0.2538\n        32       -2673.7928          -0.0964\ntraining class khong\n(2117, 1) [44, 21, 21, 24, 31, 21, 24, 24, 21, 21, 24, 27, 21, 21, 31, 24, 21, 27, 14, 24, 24, 27, 24, 34, 24, 31, 27, 21, 27, 24, 27, 31, 47, 24, 21, 24, 24, 34, 27, 17, 21, 27, 24, 24, 17, 31, 21, 24, 17, 21, 21, 27, 24, 31, 41, 31, 24, 27, 24, 24, 24, 21, 24, 21, 24, 27, 21, 24, 24, 27, 24, 17, 21, 17, 24, 31, 41, 24, 17, 21, 21, 31, 27, 17, 21] 85\n         1       -4887.6863             +nan\n         2       -4354.3973        +533.2890\n         3       -4301.6333         +52.7640\n         4       -4176.3070        +125.3263\n         5       -3966.7511        +209.5559\n         6       -3781.1852        +185.5659\n         7       -3603.2855        +177.8997\n         8       -3382.8842        +220.4013\n         9       -3183.7708        +199.1133\n        10       -3079.6846        +104.0862\n        11       -3011.1896         +68.4951\n        12       -2928.1430         +83.0466\n        13       -2823.1849        +104.9581\n        14       -2729.1168         +94.0681\n        15       -2671.9314         +57.1855\n        16       -2645.7152         +26.2162\n        17       -2633.8501         +11.8651\n        18       -2627.2304          +6.6197\n        19       -2622.8736          +4.3568\n        20       -2620.0228          +2.8507\n        21       -2617.8690          +2.1539\n        22       -2616.2126          +1.6563\n        23       -2615.2853          +0.9274\n        24       -2614.2418          +1.0435\n        25       -2612.9112          +1.3305\n        26       -2611.9056          +1.0057\n        27       -2611.3842          +0.5214\n        28       -2610.7822          +0.6020\n        29       -2610.1399          +0.6423\n        30       -2609.7097          +0.4303\n        31       -2609.4160          +0.2936\n        32       -2609.2179          +0.1981\n        33       -2609.0856          +0.1322\n        34       -2608.9986          +0.0871\n        35       -2608.9425          +0.0561\n        36       -2608.9079          +0.0346\n        37       -2608.8884          +0.0195\n        38       -2608.8798          +0.0087\n         1       -3679.8057             +nan\n         2       -3127.0727        +552.7330\ntraining class nguoi\n(1739, 1) [22, 20, 26, 32, 27, 20, 29, 20, 37, 22, 34, 25, 17, 17, 16, 18, 24, 26, 28, 20, 22, 22, 13, 21, 21, 23, 23, 21, 24, 15, 17, 18, 28, 17, 23, 19, 19, 25, 23, 32, 27, 22, 17, 18, 32, 19, 22, 28, 28, 39, 19, 22, 19, 16, 25, 17, 33, 22, 20, 28, 13, 24, 24, 28, 50, 36, 26, 18, 20, 19, 19, 20, 22, 23, 18] 75\n         3       -3085.4884         +41.5843\n         4       -2973.9517        +111.5367\n         5       -2783.6408        +190.3109\n         6       -2628.8783        +154.7625\n         7       -2477.2604        +151.6179\n         8       -2333.7198        +143.5406\n         9       -2184.7832        +148.9366\n        10       -1998.9526        +185.8306\n        11       -1870.4751        +128.4776\n        12       -1785.9155         +84.5596\n        13       -1753.1423         +32.7732\n        14       -1745.4052          +7.7371\n        15       -1742.6095          +2.7957\n        16       -1740.9472          +1.6622\n        17       -1739.7047          +1.2425\n        18       -1738.6407          +1.0640\n        19       -1737.6391          +1.0016\n        20       -1736.7082          +0.9310\n        21       -1734.8745          +1.8337\n        22       -1731.5878          +3.2867\n        23       -1728.3411          +3.2467\n        24       -1726.1289          +2.2122\n        25       -1725.3308          +0.7981\n        26       -1725.2114          +0.1194\n        27       -1725.7148          -0.5034\n         1       -5339.9299             +nan\n         2       -4761.3069        +578.6229\ntraining class trong\n(2267, 1) [27, 34, 31, 24, 27, 27, 31, 47, 24, 24, 21, 24, 31, 34, 27, 21, 37, 24, 31, 27, 31, 21, 27, 21, 24, 24, 44, 37, 34, 34, 24, 31, 24, 31, 21, 24, 24, 21, 17, 21, 24, 21, 24, 27, 27, 51, 21, 14, 31, 21, 27, 21, 24, 24, 34, 27, 24, 31, 27, 24, 31, 27, 27, 24, 24, 24, 27, 21, 31, 27, 17, 24, 31, 24, 24, 44, 24, 24, 27, 21, 57, 24, 31] 83\n         3       -4683.5789         +77.7280\n         4       -4528.9512        +154.6277\n         5       -4289.6835        +239.2677\n         6       -3943.8309        +345.8526\n         7       -3533.0269        +410.8040\n         8       -3243.8293        +289.1975\n         9       -3135.4195        +108.4098\n        10       -3065.1375         +70.2820\n        11       -3013.4951         +51.6424\n        12       -2977.0049         +36.4901\n        13       -2948.0160         +28.9889\n        14       -2922.0059         +26.0101\n        15       -2900.8321         +21.1738\n        16       -2872.2088         +28.6233\n        17       -2841.3410         +30.8679\n        18       -2813.4579         +27.8831\n        19       -2792.8922         +20.5657\n        20       -2776.2525         +16.6397\n        21       -2761.8266         +14.4260\n        22       -2747.4422         +14.3844\n        23       -2732.2602         +15.1820\n        24       -2720.6519         +11.6083\n        25       -2712.2576          +8.3942\n        26       -2708.0917          +4.1659\n        27       -2705.6406          +2.4511\n        28       -2703.2095          +2.4311\n        29       -2698.0840          +5.1254\n        30       -2671.6778         +26.4063\n        31       -2647.3353         +24.3425\n        32       -2645.6784          +1.6569\n        33       -2644.7795          +0.8989\n        34       -2644.1325          +0.6470\n        35       -2643.7553          +0.3773\n        36       -2643.5612          +0.1940\n        37       -2643.4653          +0.0960\n        38       -2643.4111          +0.0541\n        39       -2643.3489          +0.0622\n        40       -2643.1217          +0.2273\n        41       -2642.4556          +0.6660\n        42       -2641.9711          +0.4845\n        43       -2641.8598          +0.1113\n        44       -2641.8166          +0.0433\n        45       -2641.7885          +0.0281\n        46       -2641.7649          +0.0236\n        47       -2641.7418          +0.0230\n        48       -2641.7178          +0.0240\n        49       -2641.6921          +0.0257\n        50       -2641.6644          +0.0277\n        51       -2641.6345          +0.0299\n        52       -2641.6023          +0.0322\n        53       -2641.5677          +0.0346\n        54       -2641.5306          +0.0370\n        55       -2641.4913          +0.0394\n        56       -2641.4496          +0.0416\n        57       -2641.4059          +0.0438\n        58       -2641.3602          +0.0457\n        59       -2641.3127          +0.0474\n        60       -2641.2638          +0.0489\n        61       -2641.2138          +0.0500\n        62       -2641.1630          +0.0508\n        63       -2641.1118          +0.0512\n        64       -2641.0607          +0.0511\n        65       -2641.0101          +0.0506\n        66       -2640.9606          +0.0495\n        67       -2640.9126          +0.0480\n        68       -2640.8668          +0.0459\n        69       -2640.8235          +0.0433\n        70       -2640.7833          +0.0402\n        71       -2640.7464          +0.0368\n        72       -2640.7132          +0.0332\n        73       -2640.6839          +0.0293\n        74       -2640.6584          +0.0255\n        75       -2640.6367          +0.0217\n        76       -2640.6186          +0.0181\n        77       -2640.6037          +0.0148Training done\nTesting\n\n        78       -2640.5918          +0.0119\n        79       -2640.5825          +0.0094\n"
    }
   ],
   "source": [
    "models = {}\n",
    "nComp = 7\n",
    "startprobPrior,transmatPrior = initByBakis(nComp=nComp,bakisLevel=3)\n",
    "print(startprobPrior)\n",
    "print(transmatPrior)\n",
    "for cname in class_names:\n",
    "    class_vectors = dataset[cname]\n",
    "    hmm = hmmlearn.hmm.MultinomialHMM(\n",
    "        n_components=nComp, random_state=15, n_iter=1000, verbose=True,\n",
    "        startprob_prior=startprobPrior,\n",
    "        transmat_prior=transmatPrior,\n",
    "        init_params='ste',\n",
    "        params='ste'\n",
    "    )\n",
    "    if cname[:4] != 'test':\n",
    "        X = np.concatenate(dataset[cname])\n",
    "        lengths = list([len(x) for x in dataset[cname]])\n",
    "        print(\"training class\", cname)\n",
    "        print(X.shape, lengths, len(lengths))\n",
    "        hmm.fit(X, lengths=lengths)\n",
    "        models[cname] = hmm\n",
    "print(\"Training done\")\n",
    "print(\"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\nAccuracy: {'test_cothe': 1.0, 'test_duoc': 1.0, 'test_khong': 0.8666666666666667, 'test_nguoi': 1.0, 'test_trong': 1.0}\n"
    }
   ],
   "source": [
    "accuracy = {}\n",
    "for cname in class_names:\n",
    "    if cname[:4] != 'test':\n",
    "        continue\n",
    "    total_data = len(dataset[cname])\n",
    "    true_cnt = 0\n",
    "    # true result là tên chính xác của bộ test\n",
    "    true_result = class_names[class_names.index(cname) % 5]\n",
    "    for O in dataset[cname]:\n",
    "        score = {cname: model.score(O, [len(O)]) for cname, model in models.items() if cname[:4] != 'test'}\n",
    "        result = max(score, key=lambda k: score[k])\n",
    "        isTrue = true_result == result\n",
    "        # print(cname, score, result, isTrue)\n",
    "        if isTrue:\n",
    "            true_cnt += 1\n",
    "    accuracy[cname] = true_cnt/total_data\n",
    "print(\"\\n\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Saved!\n"
    }
   ],
   "source": [
    "with open(\"multinomial_hmm.pkl\", \"wb\") as file:\n",
    "    pickle.dump(models, file)\n",
    "pickle.dump(kmeans, open(\"kmeans.pkl\", \"wb\"))\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}