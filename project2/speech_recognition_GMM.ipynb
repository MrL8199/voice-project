{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import hmmlearn.hmm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(file_path):\n",
    "    y, sr = librosa.load(file_path)  # read .wav file\n",
    "    hop_length = math.floor(sr*0.010)  # 10ms hop\n",
    "    win_length = math.floor(sr*0.025)  # 25ms frame\n",
    "    # mfcc is 12 x T matrix\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y, sr, n_mfcc=12, n_fft=1024,\n",
    "        hop_length=hop_length, win_length=win_length)\n",
    "    # substract mean from mfcc --> normalize mfcc\n",
    "    mfcc = mfcc - np.mean(mfcc, axis=1).reshape((-1, 1))\n",
    "    # delta feature 1st order and 2nd order\n",
    "    delta1 = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    # X is 36 x T\n",
    "    X = np.concatenate([mfcc, delta1, delta2], axis=0)  # O^r\n",
    "    # return T x 36 (transpose of X)\n",
    "    return X.T  # hmmlearn use T x N matrix\n",
    "\n",
    "def get_class_data(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "    mfcc = [get_mfcc(os.path.join(data_dir, f))\n",
    "            for f in files if f.endswith(\".wav\")]\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def clustering(X, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50,\n",
    "                    random_state=0, verbose=0)\n",
    "    kmeans.fit(X)\n",
    "    print(\"centers\", kmeans.cluster_centers_.shape)\n",
    "    return kmeans\n",
    "\n",
    "def initByBakis(nComp, bakisLevel):\n",
    "        ''' init start_prob and transmat_prob by Bakis model ''' \n",
    "        startprobPrior = np.zeros(nComp)\n",
    "        startprobPrior[0 : bakisLevel - 1] = 1./ (bakisLevel - 1)\n",
    "         \n",
    "        transmatPrior = getTransmatPrior(nComp, bakisLevel)\n",
    "         \n",
    "        return startprobPrior, transmatPrior\n",
    "\n",
    "def getTransmatPrior(nComp, bakisLevel):\n",
    "        ''' get transmat prior '''\n",
    "        transmatPrior = (1. / bakisLevel) * np.eye(nComp)\n",
    "         \n",
    "        for i in range(nComp - (bakisLevel - 1)):\n",
    "            for j in range(bakisLevel - 1):\n",
    "                transmatPrior[i, i + j + 1] = 1. /  bakisLevel\n",
    "                 \n",
    "        for i in range(nComp - bakisLevel + 1, nComp):\n",
    "            for j in range(nComp - i -j):\n",
    "                transmatPrior[i, i + j] = 1. / (nComp - i)\n",
    "        return transmatPrior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Load cothe dataset\nLoad duoc dataset\nLoad khong dataset\nLoad nguoi dataset\nLoad trong dataset\nLoad test_cothe dataset\nLoad test_duoc dataset\nLoad test_khong dataset\nLoad test_nguoi dataset\nLoad test_trong dataset\nvectors (13684, 36)\n"
    }
   ],
   "source": [
    "class_names = [\"cothe\", \"duoc\", \"khong\", \"nguoi\", \"trong\", \"test_cothe\", \"test_duoc\", \"test_khong\", \"test_nguoi\", \"test_trong\"]\n",
    "dataset = {}\n",
    "for cname in class_names:\n",
    "    print(f\"Load {cname} dataset\")\n",
    "    dataset[cname] = get_class_data(os.path.join(\"data/cutted\", cname))\n",
    "\n",
    "# Get all vectors in the datasets\n",
    "all_vectors = np.concatenate([np.concatenate(v, axis=0) for k, v in dataset.items()], axis=0)\n",
    "print(\"vectors\", all_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "training class cothe\n(3535, 36) [44, 47, 51, 44, 47, 64, 41, 47, 41, 37, 37, 37, 37, 47, 37, 41, 37, 34, 41, 37, 47, 41, 37, 44, 47, 41, 51, 67, 37, 41, 41, 37, 37, 34, 47, 31, 67, 47, 37, 37, 44, 41, 37, 44, 34, 44, 44, 34, 47, 51, 34, 34, 34, 37, 41, 51, 27, 31, 37, 34, 47, 44, 37, 44, 34, 47, 37, 37, 44, 37, 41, 74, 44, 37, 41, 41, 41, 51, 44, 61, 37, 41, 41, 37] 84\n         1     -372215.9127             +nan\n         2     -358823.4794      +13392.4334\n         3     -355965.7090       +2857.7704\n         4     -355308.5948        +657.1143\n         5     -355098.9046        +209.6901\n         6     -355000.1154         +98.7892\n         7     -354925.7898         +74.3256\n         8     -354883.2588         +42.5311\n         9     -354868.0000         +15.2588\n        10     -354846.5584         +21.4416\n        11     -354825.9967         +20.5617\n        12     -354798.2382         +27.7585\n        13     -354785.5048         +12.7333\n        14     -354778.2273          +7.2775\n        15     -354772.7724          +5.4549\n        16     -354767.4645          +5.3079\n        17     -354760.0289          +7.4356\n        18     -354752.8889          +7.1401\n        19     -354745.7578          +7.1310\n        20     -354723.8650         +21.8929\n        21     -354624.3877         +99.4773\n        22     -354437.4538        +186.9338\n        23     -354296.8935        +140.5603\n        24     -354200.0380         +96.8555\n        25     -354134.9200         +65.1179\n        26     -354091.8272         +43.0928\n        27     -354064.0728         +27.7544\n        28     -354041.5079         +22.5648\n        29     -354016.3126         +25.1953\n        30     -353996.3416         +19.9711\n        31     -353986.8785          +9.4631\n        32     -353971.3327         +15.5458\n        33     -353953.1512         +18.1815\n        34     -353941.7736         +11.3776\n        35     -353935.2048          +6.5688\n        36     -353930.1729          +5.0319\n        37     -353921.1353          +9.0376\n        38     -353918.9322          +2.2031\n        39     -353917.8054          +1.1268\n        40     -353916.2779          +1.5275\n        41     -353913.2572          +3.0207\n        42     -353909.6922          +3.5650\n        43     -353906.8817          +2.8105\n        44     -353904.3055          +2.5762\n        45     -353901.9580          +2.3474\n        46     -353899.8945          +2.0635\n        47     -353898.8999          +0.9946\n        48     -353898.3769          +0.5230\n        49     -353898.0883          +0.2886\n        50     -353897.9248          +0.1635\n        51     -353897.8283          +0.0965\n        52     -353897.7696          +0.0587\n        53     -353897.7330          +0.0366\n        54     -353897.7098          +0.0232\n        55     -353897.6949          +0.0149\n        56     -353897.6852          +0.0097\ntraining class duoc\n(2013, 36) [21, 17, 24, 37, 27, 24, 24, 24, 17, 31, 27, 17, 21, 21, 21, 24, 14, 21, 24, 27, 24, 27, 31, 27, 24, 24, 17, 14, 24, 17, 27, 24, 24, 21, 14, 17, 24, 17, 31, 21, 27, 27, 17, 17, 17, 21, 31, 17, 24, 27, 24, 27, 24, 21, 21, 21, 41, 24, 21, 34, 57, 31, 27, 27, 21, 21, 21, 24, 21, 17, 27, 17, 21, 21, 27, 21, 27, 24, 21, 21, 24, 24, 24, 21, 31] 85\n         1     -214117.9653             +nan\n         2     -205509.2748       +8608.6906\n         3     -202803.4235       +2705.8512\n         4     -202058.1846        +745.2389\n         5     -201715.6162        +342.5685\n         6     -201511.5379        +204.0783\n         7     -201375.5893        +135.9486\n         8     -201311.5943         +63.9950\n         9     -201265.0556         +46.5387\n        10     -201231.1576         +33.8980\n        11     -201195.0517         +36.1060\n        12     -201158.7268         +36.3248\n        13     -201112.8118         +45.9150\n        14     -201085.1866         +27.6252\n        15     -201070.5003         +14.6864\n        16     -201058.4559         +12.0443\n        17     -201045.1196         +13.3364\n        18     -201023.6644         +21.4551\n        19     -200996.9339         +26.7305\n        20     -200982.6384         +14.2955\n        21     -200967.7213         +14.9171\n        22     -200955.1767         +12.5447\n        23     -200941.6326         +13.5440\n        24     -200932.3250          +9.3076\n        25     -200925.7936          +6.5314\n        26     -200918.0614          +7.7322\n        27     -200907.8361         +10.2254\n        28     -200900.2303          +7.6058\n        29     -200895.1446          +5.0857\n        30     -200891.4657          +3.6789\n        31     -200886.1789          +5.2867\n        32     -200873.6676         +12.5114\n        33     -200861.3603         +12.3073\n        34     -200845.9823         +15.3780\n        35     -200837.6408          +8.3415\n        36     -200834.8185          +2.8223\n        37     -200833.0065          +1.8120\n        38     -200821.1976         +11.8089\n        39     -200806.7123         +14.4853\n        40     -200801.8962          +4.8162\n        41     -200793.1639          +8.7323\n        42     -200787.9854          +5.1785\n        43     -200786.0044          +1.9810\n        44     -200785.1262          +0.8782\n        45     -200784.6199          +0.5064\n        46     -200784.2816          +0.3383\n        47     -200784.0305          +0.2511\n        48     -200783.8270          +0.2035\n        49     -200783.6485          +0.1785\n        50     -200783.4793          +0.1692\n        51     -200783.3037          +0.1755\n        52     -200783.0946          +0.2092\n        53     -200782.7389          +0.3557\n        54     -200781.3215          +1.4173\n        55     -200778.5040          +2.8175\n        56     -200777.4511          +1.0529\n        57     -200776.0517          +1.3995\n        58     -200774.1375          +1.9142\n        59     -200772.0007          +2.1367\n        60     -200769.1686          +2.8322\n        61     -200765.4041          +3.7645\n        62     -200761.4123          +3.9918\n        63     -200756.9101          +4.5022\n        64     -200751.5797          +5.3304\n        65     -200747.3863          +4.1934\n        66     -200743.2961          +4.0901\n        67     -200734.3117          +8.9845\n        68     -200731.4593          +2.8524\n        69     -200729.2729          +2.1864\n        70     -200728.0121          +1.2608\n        71     -200726.8609          +1.1512\n        72     -200725.0288          +1.8321\n        73     -200721.0981          +3.9307\n        74     -200718.5946          +2.5034\n        75     -200717.2123          +1.3824\n        76     -200715.3842          +1.8280\n        77     -200713.4145          +1.9698\n        78     -200712.8055          +0.6090\n        79     -200711.7272          +1.0783\n        80     -200692.7691         +18.9580\n        81     -200686.1905          +6.5786\n        82     -200685.8131          +0.3774\n        83     -200685.6471          +0.1661\n        84     -200685.4482          +0.1989\n        85     -200685.1658          +0.2824\n        86     -200684.8457          +0.3201\n        87     -200684.0327          +0.8130\n        88     -200680.4021          +3.6307\n        89     -200679.6723          +0.7298\n        90     -200679.5016          +0.1707\n        91     -200679.4182          +0.0834\n        92     -200679.3502          +0.0680\n        93     -200679.2587          +0.0915\n        94     -200679.0482          +0.2105\n        95     -200678.4350          +0.6131\n        96     -200677.4529          +0.9821\n        97     -200676.6387          +0.8142\n        98     -200676.1793          +0.4594\n        99     -200676.0700          +0.1093\n       100     -200676.0330          +0.0371\n       101     -200676.0089          +0.0241\n       102     -200675.9786          +0.0303\n       103     -200675.8836          +0.0950\n       104     -200675.0326          +0.8510\n       105     -200669.9765          +5.0560\n       106     -200667.5460          +2.4306\n       107     -200666.9382          +0.6078\n       108     -200666.7534          +0.1848\n       109     -200666.6724          +0.0810\n       110     -200666.6274          +0.0450\n       111     -200666.5979          +0.0294\n       112     -200666.5761          +0.0218\n       113     -200666.5583          +0.0178\n       114     -200666.5430          +0.0154\n       115     -200666.5292          +0.0138\n       116     -200666.5168          +0.0124\n       117     -200666.5057          +0.0110\n       118     -200666.4962          +0.0095\ntraining class khong\n(2117, 36) [44, 21, 21, 24, 31, 21, 24, 24, 21, 21, 24, 27, 21, 21, 31, 24, 21, 27, 14, 24, 24, 27, 24, 34, 24, 31, 27, 21, 27, 24, 27, 31, 47, 24, 21, 24, 24, 34, 27, 17, 21, 27, 24, 24, 17, 31, 21, 24, 17, 21, 21, 27, 24, 31, 41, 31, 24, 27, 24, 24, 24, 21, 24, 21, 24, 27, 21, 24, 24, 27, 24, 17, 21, 17, 24, 31, 41, 24, 17, 21, 21, 31, 27, 17, 21] 85\n         1     -222461.4766             +nan\n         2     -213193.6257       +9267.8510\n         3     -211673.1131       +1520.5126\n         4     -211357.7165        +315.3965\n         5     -211179.8044        +177.9121\n         6     -211020.6229        +159.1816\n         7     -210883.2455        +137.3774\n         8     -210764.9005        +118.3449\n         9     -210685.7254         +79.1751\n        10     -210628.8181         +56.9073\n        11     -210577.4050         +51.4132\n        12     -210535.2479         +42.1571\n        13     -210501.9441         +33.3038\n        14     -210470.0262         +31.9179\n        15     -210419.2557         +50.7706\n        16     -210368.9069         +50.3488\n        17     -210322.9684         +45.9385\n        18     -210303.2655         +19.7029\n        19     -210288.4091         +14.8565\n        20     -210276.7986         +11.6105\n        21     -210274.2881          +2.5105\n        22     -210273.3244          +0.9637\n        23     -210272.8033          +0.5211\n        24     -210272.5704          +0.2330\n        25     -210272.4671          +0.1033\n        26     -210272.4083          +0.0588\n        27     -210272.3700          +0.0382\n        28     -210272.3432          +0.0269\n        29     -210272.3230          +0.0202\n        30     -210272.3067          +0.0162\n        31     -210272.2929          +0.0139\n        32     -210272.2804          +0.0125\n        33     -210272.2686          +0.0118\n        34     -210272.2571          +0.0115\n        35     -210272.2455          +0.0115\n        36     -210272.2339          +0.0116\n        37     -210272.2222          +0.0116\n        38     -210272.2109          +0.0114\n        39     -210272.2002          +0.0106\n        40     -210272.1908          +0.0095\ntraining class nguoi\n(1739, 36) [22, 20, 26, 32, 27, 20, 29, 20, 37, 22, 34, 25, 17, 17, 16, 18, 24, 26, 28, 20, 22, 22, 13, 21, 21, 23, 23, 21, 24, 15, 17, 18, 28, 17, 23, 19, 19, 25, 23, 32, 27, 22, 17, 18, 32, 19, 22, 28, 28, 39, 19, 22, 19, 16, 25, 17, 33, 22, 20, 28, 13, 24, 24, 28, 50, 36, 26, 18, 20, 19, 19, 20, 22, 23, 18] 75\n         1     -172087.5343             +nan\n         2     -164656.6687       +7430.8656\n         3     -163049.1676       +1607.5011\n         4     -162676.2910        +372.8767\n         5     -162496.5097        +179.7813\n         6     -162375.0048        +121.5049\n         7     -162320.6251         +54.3797\n         8     -162269.2647         +51.3603\n         9     -162202.9046         +66.3601\n        10     -162154.9632         +47.9414\n        11     -162138.8617         +16.1015\n        12     -162130.2812          +8.5805\n        13     -162124.1041          +6.1771\n        14     -162118.4015          +5.7025\n        15     -162113.9151          +4.4864\n        16     -162107.9640          +5.9511\n        17     -162102.5976          +5.3664\n        18     -162099.8006          +2.7971\n        19     -162096.6560          +3.1445\n        20     -162094.2726          +2.3834\n        21     -162092.5843          +1.6884\n        22     -162091.1061          +1.4782\n        23     -162089.9554          +1.1506\n        24     -162089.0849          +0.8705\n        25     -162088.3304          +0.7545\n        26     -162087.6403          +0.6901\n        27     -162087.0167          +0.6236\n        28     -162086.4596          +0.5570\n        29     -162085.9507          +0.5089\n        30     -162085.4596          +0.4911\n        31     -162084.9429          +0.5167\n        32     -162084.3366          +0.6063\n        33     -162083.5770          +0.7596\n        34     -162082.6951          +0.8819\n        35     -162081.8454          +0.8498\n        36     -162081.1060          +0.7394\n        37     -162080.4081          +0.6979\n        38     -162079.4937          +0.9144\n        39     -162076.0425          +3.4512\n        40     -162070.8379          +5.2045\n        41     -162069.6880          +1.1499\n        42     -162068.8133          +0.8747\n        43     -162068.0145          +0.7988\n        44     -162067.1111          +0.9034\n        45     -162065.8907          +1.2204\n        46     -162064.4441          +1.4465\n        47     -162063.1328          +1.3113\n        48     -162061.8503          +1.2825\n        49     -162060.3164          +1.5340\n        50     -162058.0941          +2.2222\n        51     -162055.2947          +2.7994\n        52     -162053.2121          +2.0826\n        53     -162051.4076          +1.8045\n        54     -162049.5526          +1.8550\n        55     -162047.4893          +2.0633\n        56     -162045.2401          +2.2492\n        57     -162042.8652          +2.3748\n        58     -162039.6736          +3.1916\n        59     -162034.2807          +5.3930\n        60     -162028.4602          +5.8204\n        61     -162025.6681          +2.7921\n        62     -162024.3558          +1.3123\n        63     -162023.7476          +0.6081\n        64     -162023.3594          +0.3882\n        65     -162022.9771          +0.3823\n        66     -162022.5283          +0.4488\n        67     -162022.2249          +0.3034\n        68     -162022.1072          +0.1177\n        69     -162022.0378          +0.0694\n        70     -162021.9847          +0.0531\n        71     -162021.9404          +0.0443\n        72     -162021.9008          +0.0395\n        73     -162021.8636          +0.0373\n        74     -162021.8266          +0.0370\n        75     -162021.7882          +0.0384\n        76     -162021.7469          +0.0413\n        77     -162021.7017          +0.0452\n        78     -162021.6518          +0.0498\n        79     -162021.5975          +0.0544\n        80     -162021.5388          +0.0587\n        81     -162021.4753          +0.0635\n        82     -162021.4052          +0.0701\n        83     -162021.3256          +0.0796\n        84     -162021.2334          +0.0922\n        85     -162021.1268          +0.1065\n        86     -162021.0068          +0.1201\n        87     -162020.8769          +0.1298\n        88     -162020.7433          +0.1336\n        89     -162020.6120          +0.1313\n        90     -162020.4871          +0.1249\n        91     -162020.3702          +0.1169\n        92     -162020.2606          +0.1096\n        93     -162020.1556          +0.1050\n        94     -162020.0507          +0.1049\n        95     -162019.9392          +0.1115\n        96     -162019.8123          +0.1269\n        97     -162019.6615          +0.1508\n        98     -162019.4893          +0.1722\n        99     -162019.3214          +0.1679\n       100     -162019.1903          +0.1311\n       101     -162019.1015          +0.0888\n       102     -162019.0415          +0.0600\n       103     -162018.9986          +0.0429\n       104     -162018.9663          +0.0323\n       105     -162018.9410          +0.0253\n       106     -162018.9205          +0.0205\n       107     -162018.9032          +0.0173\n       108     -162018.8878          +0.0154\n       109     -162018.8734          +0.0144\n       110     -162018.8589          +0.0145\n       111     -162018.8430          +0.0159\n       112     -162018.8237          +0.0193\n       113     -162018.7975          +0.0262\n       114     -162018.7581          +0.0395\n       115     -162018.6936          +0.0644\n       116     -162018.5886          +0.1050\n       117     -162018.4430          +0.1456\n       118     -162018.3014          +0.1416\n       119     -162018.2115          +0.0899\n       120     -162018.1669          +0.0446\n       121     -162018.1439          +0.0230\n       122     -162018.1301          +0.0138\n       123     -162018.1210          +0.0091\ntraining class trong\n(2267, 36) [27, 34, 31, 24, 27, 27, 31, 47, 24, 24, 21, 24, 31, 34, 27, 21, 37, 24, 31, 27, 31, 21, 27, 21, 24, 24, 44, 37, 34, 34, 24, 31, 24, 31, 21, 24, 24, 21, 17, 21, 24, 21, 24, 27, 27, 51, 21, 14, 31, 21, 27, 21, 24, 24, 34, 27, 24, 31, 27, 24, 31, 27, 27, 24, 24, 24, 27, 21, 31, 27, 17, 24, 31, 24, 24, 44, 24, 24, 27, 21, 57, 24, 31] 83\n         1     -240827.1368             +nan\n         2     -231570.5590       +9256.5778\n         3     -229915.8479       +1654.7111\n         4     -229211.3156        +704.5323\n         5     -228748.2078        +463.1078\n         6     -228509.0356        +239.1721\n         7     -228366.7265        +142.3091\n         8     -228282.8131         +83.9135\n         9     -228227.7274         +55.0857\n        10     -228179.3797         +48.3476\n        11     -228161.5576         +17.8221\n        12     -228152.6171          +8.9405\n        13     -228141.6399         +10.9772\n        14     -228125.2506         +16.3893\n        15     -228089.5619         +35.6887\n        16     -228044.7834         +44.7785\n        17     -228007.2370         +37.5464\n        18     -227978.5447         +28.6922\n        19     -227946.8405         +31.7043\n        20     -227920.4331         +26.4074\n        21     -227912.1124          +8.3207\n        22     -227907.8110          +4.3014\n        23     -227905.0654          +2.7456\n        24     -227903.7043          +1.3612\n        25     -227902.9371          +0.7672\n        26     -227902.3859          +0.5512\n        27     -227901.8815          +0.5044\n        28     -227901.3489          +0.5325\n        29     -227900.7531          +0.5959\n        30     -227900.1087          +0.6444\n        31     -227899.4874          +0.6212\n        32     -227898.9353          +0.5521\n        33     -227898.4099          +0.5255\n        34     -227897.8602          +0.5497\n        35     -227897.3494          +0.5108\n        36     -227897.0069          +0.3425\n        37     -227896.8284          +0.1785\n        38     -227896.7240          +0.1045\n        39     -227896.6379          +0.0860\n        40     -227896.5581          +0.0798\n        41     -227896.4952          +0.0630\n        42     -227896.4531          +0.0421\n        43     -227896.4181          +0.0350\n        44     -227896.3717          +0.0464\n        45     -227896.2954          +0.0763\n        46     -227896.1722          +0.1232\n        47     -227896.0136          +0.1586\n        48     -227895.8776          +0.1360\n        49     -227895.7978          +0.0798\n        50     -227895.7575          +0.0403\n        51     -227895.7374          +0.0201\nTraining done\nTesting\n        52     -227895.7275          +0.0100\n"
    }
   ],
   "source": [
    "models = {}\n",
    "for cname in class_names:\n",
    "    class_vectors = dataset[cname]\n",
    "    nComp = 5\n",
    "    startprobPrior,transmatPrior = initByBakis(nComp=nComp,bakisLevel=3)\n",
    "    hmm = hmmlearn.hmm.GMMHMM(\n",
    "        n_components = nComp, n_mix = 2, n_iter = 1000,verbose= True,\n",
    "        params='mctw',\n",
    "        init_params='mst',\n",
    "        startprob_prior = startprobPrior,\n",
    "        transmat_prior = transmatPrior\n",
    "    )\n",
    "\n",
    "    if cname[:4] != 'test':\n",
    "        X = np.concatenate(dataset[cname])\n",
    "        lengths = list([len(x) for x in dataset[cname]])\n",
    "        print(\"training class\", cname)\n",
    "        print(X.shape, lengths, len(lengths))\n",
    "        hmm.fit(X)\n",
    "        models[cname] = hmm\n",
    "print(\"Training done\")\n",
    "print(\"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "test_cothe {'cothe': -4874.150975301159, 'duoc': -5119.845247856504, 'khong': -5090.583217308381, 'nguoi': -5219.766508002532, 'trong': -5177.750660279599} cothe True\ntest_cothe {'cothe': -3831.0171199508945, 'duoc': -4174.8089696383195, 'khong': -4076.519856679497, 'nguoi': -4262.706303501729, 'trong': -4105.946393079537} cothe True\ntest_cothe {'cothe': -6235.511049076363, 'duoc': -6685.512539274409, 'khong': -6597.440553578652, 'nguoi': -6694.72755023392, 'trong': -6629.792142858172} cothe True\ntest_cothe {'cothe': -3778.1317992628533, 'duoc': -4141.2437437448225, 'khong': -4068.194485719782, 'nguoi': -4173.037975735423, 'trong': -4126.216416527114} cothe True\ntest_cothe {'cothe': -4482.82302815465, 'duoc': -4908.406033078506, 'khong': -4791.869817945348, 'nguoi': -5049.923843899186, 'trong': -4914.050535568599} cothe True\ntest_cothe {'cothe': -4116.0346762967065, 'duoc': -4440.4364965210825, 'khong': -4401.291448089454, 'nguoi': -4496.080945303724, 'trong': -4430.194275358646} cothe True\ntest_cothe {'cothe': -3802.753399034698, 'duoc': -4228.256594007335, 'khong': -4221.861164028726, 'nguoi': -4470.238702039562, 'trong': -4247.2749349723} cothe True\ntest_cothe {'cothe': -4034.468639263706, 'duoc': -4292.448471296296, 'khong': -4336.07630852407, 'nguoi': -4579.504883236193, 'trong': -4357.682279666606} cothe True\ntest_cothe {'cothe': -3036.379944910649, 'duoc': -3401.6790803894323, 'khong': -3352.139851046154, 'nguoi': -3466.139531296579, 'trong': -3431.5317967875244} cothe True\ntest_cothe {'cothe': -4512.335890841181, 'duoc': -4901.939525380868, 'khong': -4940.61148106297, 'nguoi': -4859.030327623494, 'trong': -5004.36238393918} cothe True\ntest_cothe {'cothe': -3671.555486939988, 'duoc': -4046.5534100427517, 'khong': -3964.7706643052384, 'nguoi': -4193.888047083729, 'trong': -4045.825868562329} cothe True\ntest_cothe {'cothe': -3850.526197080373, 'duoc': -4102.166219786466, 'khong': -4161.881579371972, 'nguoi': -4395.725722604586, 'trong': -4154.082778930834} cothe True\ntest_cothe {'cothe': -4908.923351157041, 'duoc': -5189.035264296977, 'khong': -5332.96627496662, 'nguoi': -5345.676061455971, 'trong': -5320.494934575511} cothe True\ntest_cothe {'cothe': -4414.916810558341, 'duoc': -4838.088820481365, 'khong': -4754.381098303812, 'nguoi': -4841.862317227894, 'trong': -4823.620401148185} cothe True\ntest_cothe {'cothe': -5398.76274194387, 'duoc': -5967.39525926634, 'khong': -5836.523329328059, 'nguoi': -6175.9381487943065, 'trong': -5935.86799186058} cothe True\ntest_duoc {'cothe': -3430.328820865472, 'duoc': -2815.7084641568736, 'khong': -3511.1575049058506, 'nguoi': -3357.170059871276, 'trong': -3354.000056681484} duoc True\ntest_duoc {'cothe': -2739.0157318139222, 'duoc': -2387.4789389763305, 'khong': -2803.381130477589, 'nguoi': -2646.4739529282765, 'trong': -2611.7257071199374} duoc True\ntest_duoc {'cothe': -2146.4813722181552, 'duoc': -2019.6469694556547, 'khong': -2157.2038268211923, 'nguoi': -2220.7169259514594, 'trong': -2140.882013591816} duoc True\ntest_duoc {'cothe': -2317.26873257924, 'duoc': -2195.950722680051, 'khong': -2310.019202396696, 'nguoi': -2348.686416595168, 'trong': -2254.7312622122754} duoc True\ntest_duoc {'cothe': -3910.5138314840187, 'duoc': -3669.0728081305324, 'khong': -3998.5753962245876, 'nguoi': -3961.8485439319197, 'trong': -3876.8622616220173} duoc True\ntest_duoc {'cothe': -1547.8321405981412, 'duoc': -1464.0909590472322, 'khong': -1598.0156239484202, 'nguoi': -1601.2286993656887, 'trong': -1564.624149964095} duoc True\ntest_duoc {'cothe': -3628.2292240746515, 'duoc': -3543.521129901603, 'khong': -3665.8345444968904, 'nguoi': -3838.9805880421272, 'trong': -3653.197994623308} duoc True\ntest_duoc {'cothe': -2365.6032196192905, 'duoc': -2169.3825275684394, 'khong': -2345.164394469824, 'nguoi': -2444.6663008680116, 'trong': -2339.7333368988784} duoc True\ntest_duoc {'cothe': -3324.3445096219357, 'duoc': -3091.8286977383577, 'khong': -3288.2441396040886, 'nguoi': -3303.164938842825, 'trong': -3234.448753526744} duoc True\ntest_duoc {'cothe': -2693.5329558683075, 'duoc': -2484.4128815817294, 'khong': -2703.9787329441433, 'nguoi': -2782.568650423394, 'trong': -2664.434800850055} duoc True\ntest_duoc {'cothe': -1805.7781888943048, 'duoc': -1698.524718525009, 'khong': -1767.622435925917, 'nguoi': -1829.4206083957051, 'trong': -1790.6155905110725} duoc True\ntest_duoc {'cothe': -2258.5071149385717, 'duoc': -2141.0041695060604, 'khong': -2282.8602710696077, 'nguoi': -2319.0319233039786, 'trong': -2272.960285982458} duoc True\ntest_duoc {'cothe': -3703.2467414647485, 'duoc': -3219.8093673194103, 'khong': -3858.9208170666443, 'nguoi': -3898.0274244915686, 'trong': -3669.8528976981684} duoc True\ntest_duoc {'cothe': -4090.4408601350897, 'duoc': -3848.4765136545034, 'khong': -4170.089743324236, 'nguoi': -4285.990455414287, 'trong': -4086.1945457997285} duoc True\ntest_khong {'cothe': -2441.708137929254, 'duoc': -2421.3681560650025, 'khong': -2300.5597279600643, 'nguoi': -2517.3872360073765, 'trong': -2422.475596498932} khong True\ntest_khong {'cothe': -3107.401748032267, 'duoc': -3260.7774810139945, 'khong': -2796.1093207831514, 'nguoi': -3284.8050055554304, 'trong': -3185.3930467558134} khong True\ntest_khong {'cothe': -2477.429118274448, 'duoc': -2466.742854975682, 'khong': -2298.045401408659, 'nguoi': -2557.840477529504, 'trong': -2445.006107488639} khong True\ntest_khong {'cothe': -1771.6295074103034, 'duoc': -1838.9352857825995, 'khong': -1596.8258818797244, 'nguoi': -1891.4038035097055, 'trong': -1788.508817289478} khong True\ntest_khong {'cothe': -2519.651143492474, 'duoc': -2579.0707043584616, 'khong': -2254.346437115551, 'nguoi': -2762.2346680520964, 'trong': -2577.998261106639} khong True\ntest_khong {'cothe': -3006.6049120366756, 'duoc': -3031.5176832120133, 'khong': -2755.681908390718, 'nguoi': -3068.1460866576326, 'trong': -3023.8313664145835} khong True\ntest_khong {'cothe': -2949.304821895123, 'duoc': -3093.8303185083023, 'khong': -2845.044631795243, 'nguoi': -3297.6555405275044, 'trong': -2985.3803318776313} khong True\ntest_khong {'cothe': -3126.001893554089, 'duoc': -3194.60802028925, 'khong': -2934.3129927072564, 'nguoi': -3410.916850750362, 'trong': -3168.044139490255} khong True\ntest_khong {'cothe': -1901.615821267358, 'duoc': -1945.8578660252788, 'khong': -1870.1160668853595, 'nguoi': -2061.7513860455347, 'trong': -1938.5861786736054} khong True\ntest_khong {'cothe': -2111.183571172175, 'duoc': -2109.5128335628456, 'khong': -1908.8636286462288, 'nguoi': -2194.3274316085804, 'trong': -2187.6615197607057} khong True\ntest_khong {'cothe': -2093.9046921781724, 'duoc': -2060.060989222014, 'khong': -1961.482968538251, 'nguoi': -2128.40710260649, 'trong': -2037.451338612656} khong True\ntest_khong {'cothe': -1625.0735052010805, 'duoc': -1609.6942229731733, 'khong': -1523.158966094136, 'nguoi': -1694.892195306691, 'trong': -1593.4029643854049} khong True\ntest_khong {'cothe': -2478.1136144781963, 'duoc': -2571.2741139861746, 'khong': -2282.187410454014, 'nguoi': -2697.0337643999874, 'trong': -2544.828802407068} khong True\ntest_khong {'cothe': -1685.144666653158, 'duoc': -1745.881290177487, 'khong': -1532.047007895235, 'nguoi': -1759.5118013719416, 'trong': -1726.7627777048144} khong True\ntest_khong {'cothe': -2539.0529005406747, 'duoc': -2600.372803004374, 'khong': -2377.511846264372, 'nguoi': -2714.5443347315245, 'trong': -2581.4461390832194} khong True\ntest_nguoi {'cothe': -1474.2386663761038, 'duoc': -1469.893330134028, 'khong': -1465.1992136747535, 'nguoi': -1322.0341944255288, 'trong': -1485.4938178956354} nguoi True\ntest_nguoi {'cothe': -2386.867402386872, 'duoc': -2387.820262042549, 'khong': -2378.727985126194, 'nguoi': -2212.8292779561743, 'trong': -2367.342866761492} nguoi True\ntest_nguoi {'cothe': -2083.034681330874, 'duoc': -2097.948029315878, 'khong': -2175.1609564151713, 'nguoi': -1803.7081364108687, 'trong': -2150.281378831068} nguoi True\ntest_nguoi {'cothe': -1965.7834832097342, 'duoc': -2062.143444435202, 'khong': -1987.971171082259, 'nguoi': -1853.1357663650515, 'trong': -2020.6340902817408} nguoi True\ntest_nguoi {'cothe': -3447.5983146478648, 'duoc': -3427.3849907815775, 'khong': -3621.4868601925714, 'nguoi': -3167.8391058383036, 'trong': -3565.9779646417496} nguoi True\ntest_nguoi {'cothe': -1562.8394411091172, 'duoc': -1555.7079882678784, 'khong': -1580.8459507784976, 'nguoi': -1477.5901071513995, 'trong': -1544.1052676549837} nguoi True\ntest_nguoi {'cothe': -2541.4939227920127, 'duoc': -2409.1139720273695, 'khong': -2723.437131899158, 'nguoi': -2237.8001315341016, 'trong': -2601.767263091229} nguoi True\ntest_nguoi {'cothe': -1848.39468986123, 'duoc': -1902.6397846173868, 'khong': -1877.975814689044, 'nguoi': -1763.0165633436402, 'trong': -1874.8342130005092} nguoi True\ntest_nguoi {'cothe': -2265.900367052722, 'duoc': -2214.9165609740367, 'khong': -2172.510146263491, 'nguoi': -2110.2746625665754, 'trong': -2198.3753097389067} nguoi True\ntest_nguoi {'cothe': -2132.829349387881, 'duoc': -2170.1108669540013, 'khong': -2240.178936912323, 'nguoi': -1995.7933925874768, 'trong': -2196.25958098067} nguoi True\ntest_nguoi {'cothe': -2256.208956633114, 'duoc': -2261.840523066324, 'khong': -2368.015641259766, 'nguoi': -2001.6080673507804, 'trong': -2273.1110980838835} nguoi True\ntest_nguoi {'cothe': -3941.814960221856, 'duoc': -4082.869876617146, 'khong': -4117.892507107719, 'nguoi': -3738.6081089710806, 'trong': -4043.3573171809617} nguoi True\ntest_nguoi {'cothe': -2295.522011213776, 'duoc': -2328.6555173267197, 'khong': -2341.5782547103204, 'nguoi': -2052.4276716378995, 'trong': -2351.0991932548764} nguoi True\ntest_nguoi {'cothe': -1568.872486267578, 'duoc': -1573.1741935230634, 'khong': -1531.990400358644, 'nguoi': -1403.3261668272582, 'trong': -1564.3093986466981} nguoi True\ntest_nguoi {'cothe': -1734.4814861798418, 'duoc': -1729.3451893209124, 'khong': -1767.195329713913, 'nguoi': -1646.979626402759, 'trong': -1778.6937706259039} nguoi True\ntest_trong {'cothe': -2535.321768265108, 'duoc': -2463.6495364497723, 'khong': -2434.2545595941383, 'nguoi': -2852.1333602070863, 'trong': -2197.6769134976644} trong True\ntest_trong {'cothe': -2835.256877584521, 'duoc': -2764.7211082143053, 'khong': -2751.5803604584444, 'nguoi': -2924.0374510988945, 'trong': -2543.397966714742} trong True\ntest_trong {'cothe': -2134.0372097939353, 'duoc': -2078.5769425014855, 'khong': -2085.799458642047, 'nguoi': -2260.486338836138, 'trong': -1865.8666664075913} trong True\ntest_trong {'cothe': -3175.9338318677655, 'duoc': -3234.5532085213563, 'khong': -3249.1094554944225, 'nguoi': -3803.2661585350693, 'trong': -2925.0308616763} trong True\ntest_trong {'cothe': -3526.2613404999406, 'duoc': -3469.7746508456235, 'khong': -3353.491777121195, 'nguoi': -3737.4380895529202, 'trong': -3199.9929198357336} trong True\ntest_trong {'cothe': -2669.3243905126415, 'duoc': -2626.387228668659, 'khong': -2564.1034647468255, 'nguoi': -2831.6778300050505, 'trong': -2382.177601060039} trong True\ntest_trong {'cothe': -2851.918207031689, 'duoc': -2776.485177033934, 'khong': -2739.648943731292, 'nguoi': -3025.6108117249037, 'trong': -2533.609252559242} trong True\ntest_trong {'cothe': -3030.1939635695194, 'duoc': -3053.186504478376, 'khong': -2881.0781162334056, 'nguoi': -3291.899816544015, 'trong': -2756.407124186555} trong True\ntest_trong {'cothe': -2907.6198912769464, 'duoc': -2825.36101355444, 'khong': -2859.983242424928, 'nguoi': -3268.231764130305, 'trong': -2511.381668436844} trong True\ntest_trong {'cothe': -2598.219965974127, 'duoc': -2532.2828801429578, 'khong': -2549.701671510157, 'nguoi': -2738.908776066736, 'trong': -2419.6709932895174} trong True\ntest_trong {'cothe': -2754.860066129831, 'duoc': -2743.489729545225, 'khong': -2690.039256958827, 'nguoi': -2977.898149334213, 'trong': -2510.328212424797} trong True\ntest_trong {'cothe': -2281.1692319633275, 'duoc': -2207.954179601067, 'khong': -2216.143574460477, 'nguoi': -2318.823562842618, 'trong': -2080.395403829511} trong True\ntest_trong {'cothe': -3172.6247257882205, 'duoc': -3161.940306413293, 'khong': -3061.5828538258756, 'nguoi': -3569.8987535567803, 'trong': -2837.2778184301815} trong True\ntest_trong {'cothe': -2446.0521906587637, 'duoc': -2462.4307969690553, 'khong': -2365.2430864049365, 'nguoi': -2589.9308176634754, 'trong': -2303.931518560494} trong True\ntest_trong {'cothe': -2406.1194118340713, 'duoc': -2327.8105032675667, 'khong': -2332.564465143084, 'nguoi': -2606.526235809839, 'trong': -2135.275135533922} trong True\n\n\nAccuracy: {'test_cothe': 1.0, 'test_duoc': 1.0, 'test_khong': 1.0, 'test_nguoi': 1.0, 'test_trong': 1.0}\n"
    }
   ],
   "source": [
    "accuracy = {}\n",
    "for cname in class_names:\n",
    "    if cname[:4] != 'test':\n",
    "        continue\n",
    "    total_data = len(dataset[cname])\n",
    "    true_cnt = 0\n",
    "    # true result là tên chính xác của bộ test\n",
    "    true_result = class_names[class_names.index(cname) % 5]\n",
    "    for O in dataset[cname]:\n",
    "        score = {cname: model.score(O, [len(O)]) for cname, model in models.items() if cname[:4] != 'test'}\n",
    "        result = max(score, key=lambda k: score[k])\n",
    "        isTrue = true_result == result\n",
    "        print(cname, score, result, isTrue)\n",
    "        if isTrue:\n",
    "            true_cnt += 1\n",
    "    accuracy[cname] = true_cnt/total_data\n",
    "print(\"\\n\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Saved!\n"
    }
   ],
   "source": [
    "with open(\"gmm_hmm.pkl\", \"wb\") as file:\n",
    "    pickle.dump(models, file)\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}